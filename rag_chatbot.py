from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from lmstudio_llm import LMStudioLLM
import pandas as pd
import os

class RAGChatbot:
    def __init__(self, csv_path=None):
        print("üîß Initialisation du chatbot RAG avec LM Studio...")
        
        # Configuration du mod√®le LM Studio
        lm_studio_url = "http://localhost:1234/v1"
        self.llm = LMStudioLLM(base_url=lm_studio_url, temperature=0.7)
        print(f"‚úì Connexion √† LM Studio : {lm_studio_url}")
        
        # Configuration des embeddings (local)
        embeddings_model = "sentence-transformers/all-MiniLM-L6-v2"
        print(f"‚è≥ Chargement des embeddings : {embeddings_model}")
        self.embeddings = HuggingFaceEmbeddings(model_name=embeddings_model)
        print("‚úì Embeddings charg√©s")
        
        # Base de donn√©es vectorielle
        self.vectorstore = None
        self.retriever = None
        self.prompt = None
        self.df = None
        
        # Charger le CSV si fourni
        if csv_path:
            self.load_csv(csv_path)
    
    def load_csv(self, csv_path):
        """Charger et analyser le fichier CSV"""
        print(f"\nüìä Chargement du fichier CSV : {csv_path}")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Le fichier {csv_path} n'existe pas")
        
        # Lire le CSV
        self.df = pd.read_csv(csv_path)
        print(f"‚úì CSV charg√© : {len(self.df)} lignes, {len(self.df.columns)} colonnes")
        print(f"‚úì Colonnes : {', '.join(self.df.columns.tolist())}")
        
        # Cr√©er des documents textuels √† partir du CSV
        documents = self._create_documents_from_csv()
        print(f"‚úì {len(documents)} documents cr√©√©s √† partir des donn√©es")
        
        # Diviser en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len
        )
        chunks = text_splitter.split_documents(documents)
        print(f"‚úì {len(chunks)} chunks cr√©√©s")
        
        # Cr√©er la base vectorielle
        print("‚è≥ Cr√©ation de la base vectorielle...")
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory="./chroma_db"
        )
        print("‚úì Base vectorielle cr√©√©e et persist√©e")
        
        # Cr√©er la cha√Æne QA
        self._create_qa_chain()
    
    def _create_documents_from_csv(self):
        """Convertir les donn√©es CSV en documents textuels"""
        documents = []
        
        # Cr√©er un r√©sum√© g√©n√©ral
        summary = f"""
Dataset : Donn√©es sur les ventes de jeux vid√©o
Nombre total d'entr√©es : {len(self.df)}
Colonnes disponibles : {', '.join(self.df.columns.tolist())}

R√©sum√© statistique :
{self.df.describe(include='all').to_string()}
"""
        documents.append(Document(page_content=summary))
        
        # Cr√©er un document pour chaque ligne (limit√© aux 1000 premi√®res pour la performance)
        max_rows = min(1000, len(self.df))
        for idx, row in self.df.head(max_rows).iterrows():
            # Convertir chaque ligne en texte descriptif
            row_text = " | ".join([f"{col}: {row[col]}" for col in self.df.columns if pd.notna(row[col])])
            documents.append(Document(page_content=row_text))
        
        # Cr√©er des documents d'agr√©gation si des colonnes num√©riques existent
        numeric_cols = self.df.select_dtypes(include=['float64', 'int64']).columns
        if len(numeric_cols) > 0:
            agg_text = "Statistiques agr√©g√©es :\n"
            for col in numeric_cols:
                agg_text += f"{col} - Total: {self.df[col].sum():.2f}, Moyenne: {self.df[col].mean():.2f}, Max: {self.df[col].max():.2f}\n"
            documents.append(Document(page_content=agg_text))
        
        return documents
    
    def _create_qa_chain(self):
        """Cr√©er la cha√Æne de question-r√©ponse"""
        if not self.vectorstore:
            raise ValueError("Veuillez d'abord charger des donn√©es")
        
        # Template de prompt adapt√© pour les donn√©es
        prompt_template = """Tu es un assistant sp√©cialis√© dans l'analyse de donn√©es. Tu r√©ponds aux questions en te basant UNIQUEMENT sur les donn√©es fournies dans le contexte.

Contexte (donn√©es extraites) :
{context}

Question : {question}

Instructions :
- R√©ponds en fran√ßais
- Base-toi UNIQUEMENT sur les donn√©es fournies dans le contexte
- Si la r√©ponse n√©cessite des calculs, explique ton raisonnement
- Si l'information n'est pas dans les donn√©es, dis "Je ne trouve pas cette information dans les donn√©es"
- Sois pr√©cis et donne des chiffres quand c'est pertinent

R√©ponse :"""
        
        self.prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Cr√©er le retriever
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # R√©cup√©rer plus de r√©sultats pour les donn√©es
        )
        
        print("‚úì Cha√Æne QA cr√©√©e avec succ√®s\n")
    
    def ask(self, question):
        """Poser une question sur les donn√©es"""
        if not self.vectorstore:
            return {
                "answer": "Aucune donn√©e n'a √©t√© charg√©e. Veuillez charger un fichier CSV d'abord.",
                "sources": []
            }
        
        try:
            # R√©cup√©rer les documents pertinents
            relevant_docs = self.retriever.get_relevant_documents(question)
            
            # Construire le contexte
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            
            # Cr√©er le prompt complet
            full_prompt = self.prompt.format(context=context, question=question)
            
            # Obtenir la r√©ponse du mod√®le
            answer = self.llm(full_prompt)
            
            return {
                "answer": answer,
                "sources": relevant_docs
            }
        
        except Exception as e:
            return {
                "answer": f"Erreur lors de la g√©n√©ration de la r√©ponse : {e}",
                "sources": []
            }
    
    def get_data_info(self):
        """Obtenir des informations sur les donn√©es charg√©es"""
        if self.df is None:
            return "Aucune donn√©e charg√©e"
        
        info = f"""
üìä Informations sur les donn√©es :
- Nombre de lignes : {len(self.df)}
- Nombre de colonnes : {len(self.df.columns)}
- Colonnes : {', '.join(self.df.columns.tolist())}
"""
        return info