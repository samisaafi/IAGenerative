import requests
import json

class LMStudioLLM:
    """Wrapper pour utiliser LM Studio comme backend LLM via l'API OpenAI (compatible Python 3.13)"""
    
    def __init__(self, base_url="http://localhost:1234/v1", temperature=0.7, max_tokens=2000):
        """
        Initialise le client LM Studio
        
        Args:
            base_url: URL du serveur LM Studio (par d√©faut: http://localhost:1234/v1)
            temperature: Temp√©rature pour la g√©n√©ration (0.0 = d√©terministe, 1.0 = cr√©atif)
            max_tokens: Nombre maximum de tokens √† g√©n√©rer
        """
        self.base_url = base_url.rstrip('/')
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.api_endpoint = f"{self.base_url}/chat/completions"
    
    def __call__(self, prompt):
        """
        G√©n√®re une r√©ponse √† partir d'un prompt
        
        Args:
            prompt: Le prompt √† envoyer au mod√®le
            
        Returns:
            str: La r√©ponse g√©n√©r√©e par le mod√®le
        """
        try:
            # Pr√©parer la requ√™te
            headers = {
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "local-model",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": self.temperature,
                "max_tokens": self.max_tokens
            }
            
            # Envoyer la requ√™te
            response = requests.post(
                self.api_endpoint,
                headers=headers,
                json=payload,
                timeout=120  # 2 minutes timeout
            )
            
            # V√©rifier la r√©ponse
            if response.status_code == 200:
                data = response.json()
                return data['choices'][0]['message']['content']
            else:
                return f"Erreur HTTP {response.status_code}: {response.text}"
            
        except requests.exceptions.ConnectionError:
            return f"‚ùå Erreur de connexion √† LM Studio sur {self.base_url}\n\nüí° V√©rifiez que:\n   1. LM Studio est lanc√©\n   2. Un mod√®le est charg√©\n   3. Le serveur est d√©marr√© sur le port 1234"
        
        except requests.exceptions.Timeout:
            return "‚ùå Timeout: Le mod√®le met trop de temps √† r√©pondre. Essayez avec un prompt plus court."
        
        except Exception as e:
            return f"‚ùå Erreur: {str(e)}"
    
    def generate(self, prompt, **kwargs):
        """
        M√©thode alternative pour la g√©n√©ration (compatible avec certaines interfaces LangChain)
        """
        return self.__call__(prompt)